{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of $q_0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "# needed imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the likelihood \n",
    "$$\\mathcal{L}(y|\\mu+b,\\vec{\\chi}) = \\frac{e^{-(\\mu+b)}(\\mu+b)^y}{y!}\\frac{e^{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_\\mu^2}}}{\\sqrt{2\\pi\\sigma_\\mu^2}}\\frac{e^{-\\frac{(b-b_0)^2}{2\\sigma_b^2}}}{\\sqrt{2\\pi\\sigma_b^2}}$$\n",
    "Which gives (keeping only \"interesting terms\")\n",
    "$$\\ell(\\mu,b) = -(\\mu+b)+y\\ln(\\mu+b) - \\frac{1}{2}\\left(\\frac{\\mu-\\mu_0}{\\sigma_\\mu}\\right)^2 - \\frac{1}{2} \\left(\\frac{b-b_0}{\\sigma_b}\\right)^2$$\n",
    "Studying the case for $\\sigma_s\\rightarrow\\infty$ the third term above becomes zero. From this we can find the MLEs for two cases.\n",
    "\n",
    "First, finding $\\hat\\mu$, which is done by setting $\\frac{\\partial\\ell(\\mu,b)}{\\partial\\mu}=0$, this gives us $\\hat\\mu = y - b_0$. Furthermore we can find $\\hat b$ by setting $\\frac{\\partial\\ell(\\hat\\mu,b)}{\\partial b}=0$, giving $\\hat b = b_0$\n",
    "\n",
    "Then for the second case, the null hypothesis we find $\\hat{\\hat b}$ by setting $\\frac{\\partial\\ell(0,b)}{\\partial b}=0$, this yields $\\hat{\\hat{b}} =\\frac{b_0-\\sigma_b^2+\\sqrt{(b_0-\\sigma_b^2)^2+4y\\sigma_b^2}}{2}$ \n",
    "\n",
    "Using these values for the likelihood ratio test statistics gives us\n",
    "\n",
    "$$\n",
    "q_0 = -2[\\ell(s=0,\\hat{\\hat b}) - \\ell(\\hat s, \\hat b)]\n",
    "$$\n",
    "Giving \n",
    "$$\n",
    "q_0 = 2(y\\ln\\frac{y}{\\hat{\\hat b}} + \\hat{\\hat b} - y) + \\left(\\frac{\\hat{\\hat b} -b_0}{\\sigma_b}\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(mu, b, y, b_0, db):\n",
    "    ell = -(mu+b) + y*np.log(mu+b) -0.5*((b-b_0)/db)**2\n",
    "    return ell\n",
    "\n",
    "def test_Stat_null(y, b_hhat, b_0, db):\n",
    "    q_0 = 2*(y*np.log(y/b_hhat)+b_hhat - y) + ((b_hhat-b_0)/db)**2\n",
    "    return q_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the asympotics approximation of Brazzale et al we start by defining the likelihood root\n",
    "\n",
    "$$\n",
    "r(\\mu) = sign(\\hat\\mu-\\mu)\\sqrt{2(\\ell(\\hat\\mu, \\hat b) - \\ell(\\mu, \\hat{\\hat b}))} \n",
    "$$\n",
    "where $\\ell(\\mu,\\hat{\\hat b}) =\\ell_p(\\mu)$ is the profile log likelihood. For our purposes we (should) have that\n",
    "$$\n",
    "r(0) = \\sqrt{q_0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_root(mu, mu_hat, b_hat, b_hhat, y, b_0, db):\n",
    "    ell_p_hat = log_likelihood(mu_hat, b_hat, y, b_0, db)\n",
    "    ell_p = log_likelihood(mu, b_hhat, y, b_0, db)\n",
    "    r = np.sign(mu_hat- mu)*np.sqrt(2*(ell_p_hat-ell_p))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First order approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we have the first order modification of the likelihood root (NB when the dimension of $\\mu$ is 1, which is the case here)\n",
    "\\begin{equation}\n",
    "    r^*(\\mu) = r(\\mu) + \\frac{1}{r(\\mu)}\\log\\left(\\frac{q(\\mu)}{r(\\mu)}\\right)\n",
    "\\end{equation}\n",
    "where\n",
    "$$\n",
    "q(\\mu) = t(\\mu) = \\sqrt{j_p(\\hat\\mu)} (\\hat\\mu - \\mu)\n",
    "$$\n",
    "is the Wald statistic, or\n",
    "$$\n",
    "q(\\mu) = s(\\mu) = \\sqrt{j_p(\\hat\\mu)}^{-1}\\frac{\\partial\\ell_p(\\mu)}{\\partial\\mu}\n",
    "$$\n",
    "is the score statistic. And where \n",
    "$$\n",
    "j_p(\\mu) = -\\frac{\\partial^2\\ell_p(\\mu)}{\\partial\\mu^2}\n",
    "$$\n",
    "is the observed information function. \n",
    "\n",
    "For our purposes, we have that \n",
    "$$\n",
    "\\frac{\\partial\\ell_p(\\mu)}{\\partial\\mu} = -1 + \\frac{y}{\\mu + \\hat{\\hat b}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "j_p(\\mu) = -\\frac{\\partial^2\\ell_p(\\mu)}{\\partial\\mu^2} = \\frac{y}{(\\mu + \\hat{\\hat b})^2}\n",
    "$$\n",
    "\n",
    "The first order modification works models that are part the exponential family (or transformation family) making it a good approximation. Which the model we are studying in this notebook (or generally in HEP) is not. Meaning that this is not a good \"upgrade\" of the asymptotic for our purposes, as we shall soon see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_func(mu, b_hatt, y):\n",
    "    j = y/(mu+b_hatt)**2\n",
    "    return j\n",
    "\n",
    "def wald(mu_hat, mu, b_hatt, y, b_hat, db):\n",
    "    j = info_func(mu_hat, b_hatt, y)\n",
    "    t = np.sqrt(j)*(mu_hat-mu)*np.sqrt(  np.abs(y/(mu_hat + b_hat)**2 +1/db**2)/np.abs(y/(mu + b_hatt)**2 +1/db**2)   )\n",
    "    return t\n",
    "\n",
    "def score(mu_hat, mu, b_hatt, y):\n",
    "    j = info_func(mu_hat, b_hatt, y)\n",
    "    s = np.sqrt(1/j)*( -1 + y/(mu+b_hatt))\n",
    "    return s\n",
    "\n",
    "def modified_likelihood_root(mu, mu_hat, b_hat, b_hhat, y, b_0, db, doWald=True):\n",
    "    r_mu = likelihood_root(mu, mu_hat, b_hat, b_hhat, y, b_0, db)\n",
    "    if doWald == True:\n",
    "        q_mu = wald(mu_hat, mu, b_hhat, y, b_hat, db)\n",
    "    else: \n",
    "        q_mu = score(mu_hat, mu, b_hhat, y)\n",
    "    return r_mu + (1 / r_mu) * np.log(q_mu / r_mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for an example using $y=3$, $b_0=0.78$ and $\\sigma_b=0.18$, first we compare that indeed $r(0)=\\sqrt q_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8477368150191664\n",
      "1.8477368150191662\n"
     ]
    }
   ],
   "source": [
    "y_obs = 3\n",
    "b_0 = 0.78\n",
    "sigma_b = 0.18\n",
    "\n",
    "mu_hat = y_obs - b_0\n",
    "b_hatt = (b_0-sigma_b**2+np.sqrt((b_0-sigma_b**2)**2+4*y_obs*(sigma_b**2)))/2\n",
    "b_hat = b_0\n",
    "\n",
    "r = likelihood_root(0, mu_hat, b_hat, b_hatt, y_obs, b_0, sigma_b)\n",
    "\n",
    "q_0 = test_Stat_null(y_obs, b_hatt, b_0, sigma_b)\n",
    "\n",
    "print(r)\n",
    "print(np.sqrt(q_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check all the modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_s = modified_likelihood_root(0, mu_hat, b_hat, b_hatt, y_obs, b_0, sigma_b) # Wald\n",
    "r_ss = modified_likelihood_root(0, mu_hat, b_hat, b_hatt, y_obs, b_0, sigma_b, doWald=False) # Score\n",
    "r_mp = likelihood_root(0, mu_hat, b_hat, b_hatt, y_obs+0.5, b_0, sigma_b) # Mid-p\n",
    "r_s_mp = modified_likelihood_root(0, mu_hat, b_hat, b_hatt, y_obs+0.5, b_0, sigma_b)\n",
    "r_ss_mp = modified_likelihood_root(0, mu_hat, b_hat, b_hatt, y_obs+0.5, b_0, sigma_b, doWald=False)\n",
    "\n",
    "\n",
    "r_sf = stats.norm.sf(r, loc=0)\n",
    "r_star_sf = stats.norm.sf(r_s, loc=0)\n",
    "r_stars_sf = stats.norm.sf(r_ss, loc=0)\n",
    "r_sf_mp = stats.norm.sf(r_mp, loc=0)\n",
    "r_star_sf_mp = stats.norm.sf(r_s_mp, loc=0)\n",
    "r_stars_sf_mp = stats.norm.sf(r_ss_mp, loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True value</th>\n",
       "      <th>r</th>\n",
       "      <th>r* Wald</th>\n",
       "      <th>r* score</th>\n",
       "      <th>Mid-P r</th>\n",
       "      <th>Mid-P r* Wald</th>\n",
       "      <th>Mid-P r* score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p-value</th>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>0.027993</td>\n",
       "      <td>0.005342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Significance</th>\n",
       "      <td>1.9400</td>\n",
       "      <td>1.847737</td>\n",
       "      <td>1.604981</td>\n",
       "      <td>2.319975</td>\n",
       "      <td>2.159381</td>\n",
       "      <td>1.911152</td>\n",
       "      <td>2.552854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              True value         r   r* Wald  r* score   Mid-P r  \\\n",
       "p-value           0.0264  0.032320  0.054249  0.010171  0.015410   \n",
       "Significance      1.9400  1.847737  1.604981  2.319975  2.159381   \n",
       "\n",
       "              Mid-P r* Wald  Mid-P r* score  \n",
       "p-value            0.027993        0.005342  \n",
       "Significance       1.911152        2.552854  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making a table of the results...\n",
    "\n",
    "data_sf = {'True value': [0.0264, 1.94], 'r': [r_sf, r], 'r* Wald': [r_star_sf, r_s], 'r* score': [r_stars_sf, r_ss], \n",
    "        'Mid-P r': [r_sf_mp, r_mp], 'Mid-P r* Wald': [r_star_sf_mp, r_s_mp], 'Mid-P r* score': [r_stars_sf_mp, r_ss_mp]}\n",
    "df= pd.DataFrame(data_sf)\n",
    "df.index = ['p-value', 'Significance']\n",
    "\n",
    "df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Higher order density approximations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better results we need to go to higher order approximations of the asymptotic, which means modifying Eq. (1) wrt to these values for $q$:\n",
    "$$\n",
    "q_1 = \\frac{|\\ell_{;\\hat\\theta}(\\hat\\theta) - \\ell_{;\\hat\\theta}(\\hat{\\hat{\\theta}}) \\qquad \\ell_{b;\\hat\\theta}(\\hat{\\hat{\\theta}})|}{|\\ell_{\\theta;\\hat\\theta}(\\hat\\theta)|}\\left\\{\\frac{|j_{\\theta\\theta}(\\hat{\\theta})|}{|j_{bb}(\\hat{\\theta}_\\psi)|}\\right\\}^{1/2}\n",
    "$$\n",
    "where $|\\cdot|$ denotes the determinant, and we are using the notation\n",
    "$$\n",
    "\\ell_{;X}(Z) = \\frac{\\partial\\ell(\\theta)}{\\partial X}\\Big|_{\\theta=Z}\\quad\\text{and}\\qquad \\ell_{Y;\\ell_\\theta(\\hat\\theta)}(Z) = \\frac{\\partial\\ell(\\theta)}{\\partial Y\\partial \\ell_\\theta(\\hat\\theta)^T}\\Big|_{\\theta=Z}\n",
    "$$\n",
    "with $\\theta=(\\mu,b)$ as a shorthand notation and where $\\hat\\theta = (\\hat\\mu,\\hat b)$ and $\\hat{\\hat{\\theta}} = (\\mu, \\hat{\\hat b})$\n",
    "\n",
    "\n",
    "\n",
    "**Alternatively** we could use\n",
    "$$\n",
    "q_2 = \\frac{|\\varphi(\\hat\\theta) - \\varphi(\\hat{\\hat{\\theta}}) \\qquad \\varphi_{\\lambda}(\\hat{\\hat{\\theta}})|}{|\\varphi_{\\theta}(\\hat\\theta)|}\\left\\{\\frac{|j_{\\theta\\theta}(\\hat{\\theta})|}{|j_{\\lambda\\lambda}(\\hat{\\theta}_\\psi)|}\\right\\}^{1/2}\n",
    "$$\n",
    "which uses the canonical parameter, $\\varphi$, of the tangent exponential model (TEM). Meaning that we have to transform our likelihood. Which is undesirable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculations to get the final result...** $q_1$ Edition\n",
    "Before anything we have to transform the log-likelihood $\\ell(\\theta,y)$ to be a function of the MLE's and some ancilliary statistics $\\ell(\\theta;\\hat\\theta,a)$. We repeat the log-likelhood\n",
    "$$\n",
    "\\ell(\\theta,y) =  -(\\mu+b)+y\\ln(\\mu+b) -\\frac{1}{2}\\left(\\frac{b-b_0}{\\sigma_b}\\right)^2\n",
    "$$\n",
    "We know from the MLEs that we have\n",
    "$$\n",
    "\\hat\\mu = y-b_0,\\quad\\text{and}\\qquad \\hat b =b_0\n",
    "$$\n",
    "meaning that we can rewrite\n",
    "$$\n",
    "y = \\hat\\mu+\\hat b \n",
    "$$\n",
    "***which is what we expect when $y$ is from a Poisson counting experiment***, additionally we can in this example find the simplest ancilliary statistic choice by setting\n",
    "$$\n",
    "a=y-(\\hat\\mu+\\hat b) = 0\n",
    "$$\n",
    "Thus we can rewrite the log-likelihood to\n",
    "$$\n",
    "\\ell(\\theta;\\hat\\theta,a) =  -(\\mu+b)+(\\hat\\mu + \\hat b +a)\\ln(\\mu+b) -\\frac{1}{2}\\left(\\frac{b-\\hat{b}}{\\sigma_b}\\right)^2\n",
    "$$\n",
    "or since we know $a$\n",
    "$$\n",
    "\\boxed{\\ell(\\theta;\\hat\\theta,a) =  -(\\mu+b)+(\\hat\\mu + \\hat b)\\ln(\\mu+b) -\\frac{1}{2}\\left(\\frac{b-\\hat{b}}{\\sigma_b}\\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculating $\\ell_{;\\hat\\theta}(\\hat\\theta)$**\n",
    "Starting the calculations... we have first that\n",
    "$$\n",
    "\\ell_{;\\hat\\theta}(\\hat\\theta) = \\begin{pmatrix} \\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat\\mu}\\Big|_{\\mu=y-b_0,b=b_0}\\\\\n",
    "\\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat b}\\Big|_{\\mu=y-b_0,b=b_0}\\end{pmatrix}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat\\mu}\\Big|_{\\mu=y-b_0,b=b_0} = \\frac{\\partial}{\\partial\\hat\\mu}\\left( -(\\mu+b)+(\\hat\\mu+\\hat{b})\\ln(\\mu+b) -\\frac{1}{2}\\left(\\frac{b-\\hat{b}}{\\sigma_b}\\right)^2\\right)\\Big|_{\\mu=y-b_0,b=b_0} = \\log(\\mu+b)\\Big|_{\\mu=y-b_0,b=b_0} = \\log y\n",
    "$$\n",
    "Similarly for $b$ we have\n",
    "$$\n",
    "\\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat b}\\Big|_{\\mu=y-b_0,b=b_0} = \\frac{\\partial}{\\partial\\hat\\mu}\\left( -(\\mu+b)+(\\hat\\mu+\\hat b)\\ln(\\mu+b) -\\frac{1}{2}\\left(\\frac{b-\\hat b}{\\sigma_b}\\right)^2\\right)\\Big|_{\\mu=y-b_0,b=b_0} = \\log(\\mu+b) + \\frac{b-\\hat b}{\\sigma_b^2}\\Big|_{\\mu=y-b_0,b=b_0} = \\log(y)\n",
    "$$\n",
    "Giving \n",
    "$$\n",
    "\\boxed{\\ell_{;\\hat\\theta}(\\hat\\theta) = \\begin{pmatrix} \\log y\\\\ \\log y\\end{pmatrix}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Calculating $\\ell_{;\\hat\\theta}(\\hat{\\hat{\\theta}})$**\n",
    "Next we have that\n",
    "$$\n",
    "\\ell_{;\\hat\\theta}(\\hat{\\hat{\\theta}}) = \\begin{pmatrix} \\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat\\mu}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}}\\\\\n",
    "\\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat b}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}}\\end{pmatrix}\n",
    "$$\n",
    "using the previous results we get \n",
    "$$\n",
    "\\boxed{\\ell_{;\\hat\\theta}(\\hat{\\hat{\\theta}}) = \\begin{pmatrix} \\log(\\mu+\\hat{\\hat b})\\\\  \\log(\\mu+\\hat{\\hat b}) +\\frac{\\hat{\\hat b}-\\hat{b}}{\\sigma_b^2}\\end{pmatrix}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Calculating $\\ell_{b;\\hat\\theta}(\\hat{\\hat{\\theta}})$**\n",
    "Next we have that\n",
    "$$\n",
    "\\ell_{b;\\hat\\theta}(\\hat{\\hat{\\theta}}) = \\begin{pmatrix} \\frac{\\partial^2\\ell(\\mu,b)}{\\partial b \\partial\\hat\\mu^T}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}}\\\\\n",
    "\\frac{\\partial^2\\ell(\\mu,b)}{\\partial b\\partial\\hat b}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}}\\end{pmatrix}\n",
    "$$\n",
    "Using the previous results, we have that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial b}\\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat\\mu}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}} = \\frac{\\partial}{\\partial b}\\log(\\mu+b)\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}} = \\frac{1}{\\mu + b}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}} = \\frac{1}{\\mu + \\hat{\\hat b}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b}\\frac{\\partial\\ell(\\mu,b)}{\\partial\\hat b}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}} = \\frac{\\partial}{\\partial b}\\log(\\mu+b) +\\frac{b-\\hat b}{\\sigma_b^2}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}} = \\frac{1}{\\mu + \\hat{\\hat b}} + \\frac{1}{\\sigma_b^2}\n",
    "$$\n",
    "giving\n",
    "$$\n",
    "\\boxed{\\ell_{b;\\hat\\theta}(\\hat{\\hat{\\theta}}) = \\begin{pmatrix} \\frac{1}{\\mu + \\hat{\\hat b}}\\\\  \\frac{1}{\\mu + \\hat{\\hat b}} + \\frac{1}{\\sigma_b^2}\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculating $\\ell_{\\theta;\\hat\\theta}(\\hat\\theta)$**\n",
    "Starting the calculations... we have first that\n",
    "$$\n",
    "\\ell_{\\theta;\\hat\\theta}(\\hat\\theta) = \\begin{pmatrix} \\frac{\\partial^2\\ell(\\mu,b)}{\\partial \\mu \\partial\\hat\\mu}\\Big|_{\\mu=y-b_0,b=b_0} &\n",
    "\\frac{\\partial^2\\ell(\\mu,b)}{\\partial b \\partial\\hat\\mu}\\Big|_{\\mu=y-b_0,b=b_0}\\\\\n",
    "\\frac{\\partial^2\\ell(\\mu,b)}{\\partial\\mu\\partial\\hat b}\\Big|_{\\mu=y-b_0,b=b_0} &\n",
    "\\frac{\\partial^2\\ell(\\mu,b)}{\\partial b\\partial\\hat b}\\Big|_{\\mu=y-b_0,b=b_0}\\end{pmatrix}\n",
    "$$\n",
    "We almost have all of these terms! Starting by finding the easiest term\n",
    "$$\n",
    "\\frac{\\partial^2\\ell(\\mu,b)}{\\partial \\mu \\partial\\hat b}\\Big|_{\\mu=y-b_0,b=b_0} = \\frac{1}{\\mu+b}\\Big|_{\\mu=y-b_0,b=b_0} = \\frac{1}{y}\n",
    "$$\n",
    "Next we gotta find\n",
    "$$\n",
    "\\frac{\\partial^2\\ell(\\mu,b)}{\\partial \\mu \\partial\\hat\\mu}\\Big|_{\\mu=y-b_0,b=b_0} = \\frac{1}{\\mu+b}\\Big|_{\\mu=y-b_0,b=b_0} = \\frac{1}{y}\n",
    "$$\n",
    "Such that we have\n",
    "$$\n",
    "\\boxed{\\ell_{\\theta;\\hat\\theta}(\\hat\\theta) = \\begin{pmatrix} \\frac{1}{y} &\n",
    "\\frac{1}{y}\\\\\n",
    "\\frac{1}{y} &\n",
    "\\frac{1}{y} + \\frac{1}{\\sigma_b^2}\\end{pmatrix} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Jacobian temrs**\n",
    "Lastly we have the Jacobian terms. Where we can identify that\n",
    "$$\n",
    "j_{\\theta\\theta}(\\hat\\theta) = -\\ell_{\\theta;\\hat\\theta}(\\hat\\theta) \n",
    "$$\n",
    "giving\n",
    "$$\n",
    "\\Rightarrow |j_{\\theta\\theta}(\\hat\\theta)| = \\frac{1}{y\\sigma_b^2}\n",
    "$$\n",
    "\n",
    "The other term we already have, it just has to be evaluated for other values:\n",
    "$$\n",
    "j_{bb}(\\hat{\\hat{\\theta}}) =  -\\frac{\\partial^2\\ell(\\mu,b)}{\\partial b\\partial b^T}\\Big|_{\\mu=\\mu,b=\\hat{\\hat b}}  = \\frac{y}{(\\mu+\\hat{\\hat b})^2} + \\frac{1}{\\sigma_b^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculating the determinants**\n",
    "Lastly comes the tedious part of calculating the determinant of the matrices we will get. First we have\n",
    "$$\n",
    "|\\ell_{;\\hat\\theta}(\\hat\\theta) - \\ell_{;\\hat\\theta}(\\hat{\\hat{\\theta}}) \\qquad \\ell_{b;\\hat\\theta}(\\hat{\\hat{\\theta}})| = \n",
    "\\left|\\begin{vmatrix}\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right) &\n",
    "\\frac{1}{\\mu + \\hat{\\hat b}}\\\\  \n",
    "\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right) - \\frac{\\hat{\\hat b}-\\hat{b}}{\\sigma_b^2}&\n",
    "\\frac{1}{\\mu + \\hat{\\hat b}} + \\frac{1}{\\sigma_b^2} \\end{vmatrix}\\right| = \\left| \\frac{\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right)}{\\sigma_b^2} - \\frac{(\\hat{\\hat b}-\\hat{b})}{(\\mu + \\hat{\\hat b})\\sigma_b^2} \\right|\n",
    "$$\n",
    "and\n",
    "$$\n",
    "|\\ell_{\\theta;\\hat\\theta}(\\hat\\theta)| = \\frac{1}{y\\sigma_b^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "We now have a formula for the higher order approximation\n",
    "$$\n",
    "q_1 = \\left| y\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right) - y\\frac{(\\hat{\\hat b}-\\hat{b})}{\\mu + \\hat{\\hat b}} \\right|\n",
    "\\left[\\frac{\\frac{1}{y\\sigma_b^2}}{\\frac{y}{(\\mu+\\hat{\\hat b})^2} + \\frac{1}{\\sigma_b^2}}\\right]^{1/2}\n",
    "$$\n",
    "So now we can test this result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_order_terms(mu, b_hat, b_hhat, y, db):\n",
    "        q = np.abs(np.log(y/(mu+b_hhat))*y -y*(b_hhat-b_hat)/((mu+b_hhat)))\n",
    "        jacobian = np.sqrt(  np.abs(1/(y*db**2)) / np.abs(1/db**2 + (y/(mu+b_hhat)**2)))\n",
    "        return q*jacobian\n",
    "\n",
    "def modified_likelihood_root_HO(mu, mu_hat, b_hat, b_hhat, y, b_0, db):\n",
    "        r_mu = likelihood_root(mu, mu_hat, b_hat, b_hhat, y, b_0, db)\n",
    "        q_1 = higher_order_terms(mu, b_hat, b_hhat, y, db)\n",
    "        return r_mu + (1 / r_mu) * np.log(q_1 / r_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True value</th>\n",
       "      <th>r</th>\n",
       "      <th>r*</th>\n",
       "      <th>r* H.O.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p-value</th>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>0.031624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Significance</th>\n",
       "      <td>1.9400</td>\n",
       "      <td>1.847737</td>\n",
       "      <td>1.604981</td>\n",
       "      <td>1.857450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              True value         r        r*   r* H.O.\n",
       "p-value           0.0264  0.032320  0.054249  0.031624\n",
       "Significance      1.9400  1.847737  1.604981  1.857450"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_s_HO = modified_likelihood_root_HO(0, mu_hat, b_hat, b_hatt, y_obs, b_0, sigma_b)\n",
    "\n",
    "r_sf_HO = stats.norm.sf(r_s_HO, loc=0)\n",
    "\n",
    "#Making a table of the results...\n",
    "\n",
    "data_sf = {'True value': [0.0264, 1.94], 'r': [r_sf, r], 'r*': [r_star_sf, r_s], 'r* H.O.': [r_sf_HO, r_s_HO]}\n",
    "df= pd.DataFrame(data_sf)\n",
    "df.index = ['p-value', 'Significance']\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Skovgaard's approximation**\n",
    "As the previous result would require the user to transform the log-likelihood, it is not so practical for physicists to use on the fly. Thus we can use an alternative method of finding the Higher order approximation by, Skovgaard approximation: \n",
    "$$ \n",
    "q_1=[\\hat{S}^{-1}\\hat{Q}]_1|j(\\hat{\\theta})|^{1/2}|i^{-1}(\\hat{\\theta})||\\hat{S}||j_{bb}(\\hat{\\theta}_\\psi)|^{-1/2} \n",
    "$$ \n",
    "Where \n",
    "$$\\hat{S} = S(\\hat{\\theta},\\hat{\\theta}_\\psi) = cov_{\\hat\\theta}\\{\\ell_\\theta(\\hat\\theta),\\ell_\\theta(\\hat{\\hat{\\theta}})\\}$$ \n",
    "and \n",
    "$$\\hat{Q} = Q(\\hat{\\theta},\\hat{\\theta}_\\psi)= cov_{\\hat\\theta}\\{\\ell_\\theta(\\hat\\theta),\\ell(\\hat\\theta) - \\ell(\\hat{\\hat{\\theta}})\\}$$ \n",
    "\n",
    "All of these variables are easily accessible using standard statistical software. A caveat of using Skovgaard's approximation is that it requires generating more samples $y$ and $b_0$ around $\\hat\\theta$. Below we showcase one way of implementing this numerically using JAX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Needed packages\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import grad, hessian\n",
    "from scipy.optimize import minimize\n",
    "# np.random.seed(42)  # Ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the MLEs numerically, just to show that is possible to generalize this completely for any log-likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MLEs: [2.22000001 0.77999999] \n",
      "Analytical MLEs: [2.2199999999999998, 0.78]\n",
      "Estimated profile MLEs: [0.        0.8605509] \n",
      "Analytical profile MLEs: [0, np.float64(0.8605509013859143)]\n"
     ]
    }
   ],
   "source": [
    "# Find MLE using scipy\n",
    "# Define the log-likelihood function using JAX-compatible functions\n",
    "def log_likelihood(theta, y, lamda0, sigma_lamda):\n",
    "    psi, lamda = theta\n",
    "    return -(psi + lamda) + y * jnp.log(psi + lamda) - 0.5 * ((lamda - lamda0) / sigma_lamda) ** 2\n",
    "\n",
    "# Define the log-likelihood function for MLE search\n",
    "def log_likelihood_np(theta, y, lamda0, sigma_lamda):\n",
    "    psi, lamda = theta\n",
    "    return -(psi + lamda) + y * np.log(psi + lamda) - 0.5 * ((lamda - lamda0) / sigma_lamda) ** 2\n",
    "\n",
    "# Define the profile log-likelihood function for MLE search\n",
    "def profile_likelihood(lamda, psi_fixed, y, lamda0, sigma_lamda):\n",
    "    return -log_likelihood_np([psi_fixed, lamda], y, lamda0, sigma_lamda)\n",
    "\n",
    "mu = 0 # background hypothesis\n",
    "# Find MLE using scipy\n",
    "initial_guess = [1.0, 1.0]\n",
    "mle_result = minimize(lambda theta: -log_likelihood_np(theta, y_obs, b_0, sigma_b), initial_guess)\n",
    "hat_theta = mle_result.x\n",
    "\n",
    "mle_profile_result = minimize(lambda lamda: profile_likelihood(lamda, mu, y_obs, b_0, sigma_b), [hat_theta[1]])\n",
    "hat_theta_psi = jnp.array([mu, mle_profile_result.x[0]])\n",
    "\n",
    "print(f'Estimated MLEs: {hat_theta} \\nAnalytical MLEs: {[mu_hat, b_hat]}')\n",
    "print(f'Estimated profile MLEs: {hat_theta_psi} \\nAnalytical profile MLEs: {[mu, b_hatt]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize all of the JAX functions we need to vectorize this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use automatic differentiation for gradients and Hessians\n",
    "gradient = grad(log_likelihood, argnums=0)  # Gradient w.r.t theta\n",
    "hessian_func = hessian(log_likelihood, argnums=0)  # Hessian w.r.t theta\n",
    "\n",
    "grad_vmap = jax.vmap(lambda y, b_0: gradient(hat_theta, y, b_0, sigma_b))     \n",
    "grad_vmap_psi = jax.vmap(lambda y, b_0: gradient(hat_theta_psi, y, b_0, sigma_b))\n",
    "\n",
    "ll_vmap = jax.vmap(lambda y, b_0: log_likelihood(hat_theta, y, b_0, sigma_b))\n",
    "ll_vmap_psi = jax.vmap(lambda y, b_0: log_likelihood(hat_theta_psi, y, b_0, sigma_b))\n",
    "\n",
    "info_vmap = jax.vmap(lambda y, b_0: hessian_func(hat_theta, y, b_0, sigma_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have created four functions of the samples $y$ and $b_0$\n",
    "$$\n",
    "\\ell_\\theta(\\hat\\theta, y, b_0),\\quad\\ell_\\theta(\\hat{\\hat{\\theta}}, y, b_0),\\quad\\ell(\\hat\\theta, y,b_0),\\quad\\text{and}\\quad\\ell(\\hat{\\hat{\\theta}}, y,b_0),\n",
    "$$\n",
    "respectively for all \"vmap\"s\n",
    "\n",
    "Next we compute $j(\\hat\\theta)$ and $j_{bb}(\\hat{\\hat{\\theta}})$, which is easily computed as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33333334  0.33333334]\n",
      " [ 0.33333334 31.197529  ]]\n",
      "34.91525\n"
     ]
    }
   ],
   "source": [
    "# Compute fisher info using JAX\n",
    "j_hat = -hessian_func(hat_theta, y_obs, b_0, sigma_b)\n",
    "j_bb = -hessian_func(hat_theta_psi, y_obs, b_0, sigma_b)[-1,-1]\n",
    "\n",
    "print(j_hat)\n",
    "print(j_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we need to generate more samples to compute the expected fisher information $i(\\hat\\theta)=E[j(\\hat\\theta)]$, $\\hat Q$, and $\\hat S$. To do this we generate samples that are centered around $\\hat\\theta$. This we do by computing new NPs with $b_0 \\sim\\mathcal{N}(\\hat{b}, \\sigma_b^2)$ and new observations from $y\\sim\\text{Poisson}(\\hat\\mu + \\hat b)$. For this example we generate 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boots = 100000\n",
    "b_bootstrap = np.random.normal(loc=hat_theta[1], scale=sigma_b, size=n_boots)\n",
    "y_bootstrap = np.random.poisson(lam=hat_theta[0] + hat_theta[1], size=n_boots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate $\\hat S$. This we do by calculating $\\ell_\\theta(\\hat\\theta, y)$ and $\\ell_\\theta(\\hat{\\hat{\\theta}}, y)$ for every generated \"y_bootstrap\" and \"b_bootstrap\" sample we generated. Then we calculate the covariance between the two scores, but we remember that we have\n",
    "\n",
    "$$\n",
    "\\text{cov}(\\ell_\\theta(\\hat\\theta), \\ell_\\theta(\\hat{\\hat{\\theta}})) \n",
    "= \n",
    "\\begin{pmatrix}\n",
    "    \\text{Cov}(\\ell_\\theta(\\hat\\theta), \\ell_\\theta(\\hat\\theta)) & \\text{Cov}(\\ell_\\theta(\\hat\\theta), \\ell_\\theta(\\hat{\\hat{\\theta}})) \\\\\n",
    "    \\text{Cov}(\\ell_\\theta(\\hat{\\hat{\\theta}}), \\ell_\\theta(\\hat\\theta)) & \\text{Cov}(\\ell_\\theta(\\hat{\\hat{\\theta}}), \\ell_\\theta(\\hat{\\hat{\\theta}}))\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "    \\text{Var}(\\ell_\\mu(\\hat\\mu)) & \\text{Cov}(\\ell_\\mu(\\hat\\mu), \\ell_b(\\hat b)) & \\text{Cov}(\\ell_\\mu(\\hat\\mu), \\ell_\\mu(\\mu)) &\\text{Cov}(\\ell_\\mu(\\hat\\mu), \\ell_b(\\hat{\\hat b}) \\\\\n",
    "    \\text{Cov}(\\ell_b(\\hat b), \\ell_\\mu(\\hat\\mu)) & \\text{Var}(\\ell_b(\\hat b)) & \\text{Cov}(\\ell_b(\\hat b), \\ell_\\mu(\\mu)) &\\text{Cov}(\\ell_b(\\hat b), \\ell_b(\\hat{\\hat b}) \\\\\n",
    "    \\text{Cov}(\\ell_\\mu(\\mu), \\ell_\\mu(\\hat\\mu)) & \\text{Cov}(\\ell_\\mu(\\mu), \\ell_b(\\hat b)) & \\text{Var}(\\ell_\\mu(\\mu)) &\\text{Cov}(\\ell_\\mu(\\mu), \\ell_b(\\hat{\\hat b}) \\\\\n",
    "    \\text{Cov}(\\ell_b(\\hat{\\hat b}), \\ell_\\mu(\\hat\\mu)) & \\text{Cov}(\\ell_b(\\hat{\\hat b}), \\ell_b(\\hat b)) & \\text{Cov}(\\ell_b(\\hat{\\hat b}), \\ell_\\mu(\\mu)) &\\text{Var}(\\ell_b(\\hat{\\hat b})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "So we are interested in the upper right four elements. To get $\\hat S$ we use the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33322312  0.32118786  1.16166214  1.14962687]\n",
      " [ 0.32118786 30.99279018  1.11970554 31.7913077 ]\n",
      " [ 1.16166214  1.11970554  4.04971578  4.00775914]\n",
      " [ 1.14962687 31.7913077   4.00775914 34.64943977]]\n",
      "[[ 1.16166214  1.14962687]\n",
      " [ 1.11970554 31.7913077 ]]\n"
     ]
    }
   ],
   "source": [
    "score_vectors = grad_vmap(y_bootstrap, b_bootstrap)\n",
    "score_vectors_psi = grad_vmap_psi(y_bootstrap, b_bootstrap)\n",
    "\n",
    "# showing the full covariance matrix\n",
    "S_hat = np.cov(score_vectors.T, score_vectors_psi.T)\n",
    "print(S_hat)\n",
    "\n",
    "# the part that is of interest to us\n",
    "S_hat = S_hat[:len(hat_theta), len(hat_theta):]\n",
    "print(S_hat)\n",
    "\n",
    "S_hat += 1e-8 * np.eye(S_hat.shape[0])  # Regularization for numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate $\\hat Q$, which follows the same procedure as described above,\n",
    "$$\n",
    "\\text{cov}(\\ell_\\theta(\\hat\\theta), \\Delta\\ell) \n",
    "= \n",
    "\\begin{pmatrix}\n",
    "    \\text{Var}(\\ell_\\mu(\\hat\\mu)) & \\text{Cov}(\\ell_\\mu(\\hat\\mu), \\ell_b(\\hat b)) & \\text{Cov}(\\ell_\\mu(\\hat\\mu), \\Delta\\ell) \\\\\n",
    "    \\text{Cov}(\\ell_b(\\hat b), \\ell_\\mu(\\hat\\mu)) & \\text{Var}(\\ell_b(\\hat b)) & \\text{Cov}(\\ell_b(\\hat b), \\Delta\\ell) \\\\\n",
    "    \\text{Cov}(\\Delta\\ell, \\ell_\\mu(\\hat\\mu)) & \\text{Cov}(\\Delta\\ell, \\ell_b(\\hat b)) & \\text{Var}(\\Delta\\ell)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where to spare notation I have written: $\\Delta\\ell = \\ell(\\hat\\theta)-\\ell(\\hat{\\hat{\\theta}})$\n",
    "\n",
    "\n",
    "We compute this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.04971578  4.00775914  4.35541282]\n",
      " [ 4.00775914 34.64943977  1.83872917]\n",
      " [ 4.35541282  1.83872917  4.88327212]]\n",
      "[4.35541282 1.83872917]\n"
     ]
    }
   ],
   "source": [
    "ll_bootstrap_mle = ll_vmap(y_bootstrap, b_bootstrap)\n",
    "ll_bootstrap_mle_psi = ll_vmap_psi(y_bootstrap, b_bootstrap)\n",
    "\n",
    "ll_diffs = ll_bootstrap_mle - ll_bootstrap_mle_psi\n",
    "\n",
    "Q_hat = np.cov(score_vectors_psi.T, ll_diffs)  # Covariance between score vector and scalar of differences of log-likelihoods\n",
    "\n",
    "print(Q_hat)\n",
    "\n",
    "Q_hat = Q_hat[:-1, -1]  # Covariance between score vector and scalar\n",
    "\n",
    "print(Q_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we find the expected Fisher information, and inverse $\\hat S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33371446  0.33371446]\n",
      " [ 0.33371446 31.197906  ]]\n",
      "[[ 0.891924   -0.03225346]\n",
      " [-0.03141401  0.03259112]]\n"
     ]
    }
   ],
   "source": [
    "j_hats = -info_vmap(y_bootstrap, b_bootstrap)\n",
    "i_hat = np.mean(j_hats, axis=0)\n",
    "\n",
    "S_inv =jnp.linalg.inv(S_hat)\n",
    "print(i_hat)\n",
    "\n",
    "print(S_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this we now can compute $q_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.185977\n"
     ]
    }
   ],
   "source": [
    "q_1 = np.abs(S_inv @ Q_hat)[0] * jnp.sqrt(jnp.abs(jnp.linalg.det(j_hat))) \\\n",
    "       * jnp.abs(jnp.linalg.det(jnp.linalg.inv(i_hat))) * jnp.abs(jnp.linalg.det(S_hat)) / jnp.sqrt(jnp.abs(j_bb))\n",
    "\n",
    "print(q_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with this we get the modified likelihood-root. The results are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True value</th>\n",
       "      <th>r</th>\n",
       "      <th>r*</th>\n",
       "      <th>r* H.O.</th>\n",
       "      <th>r* Skovgaard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p-value</th>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Significance</th>\n",
       "      <td>1.9400</td>\n",
       "      <td>1.847737</td>\n",
       "      <td>1.604981</td>\n",
       "      <td>1.857450</td>\n",
       "      <td>2.582782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              True value         r        r*   r* H.O.  r* Skovgaard\n",
       "p-value           0.0264  0.032320  0.054249  0.031624      0.004900\n",
       "Significance      1.9400  1.847737  1.604981  1.857450      2.582782"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_skov = r + 1/r*np.log(q_1/r)\n",
    "\n",
    "r_sf_skov = stats.norm.sf(r_skov, loc=0)\n",
    "\n",
    "#Making a table of the results...\n",
    "\n",
    "data_sf = {'True value': [0.0264, 1.94], 'r': [r_sf, r], 'r*': [r_star_sf, r_s], 'r* H.O.': [r_sf_HO, r_s_HO], 'r* Skovgaard': [r_sf_skov, r_skov]}\n",
    "df= pd.DataFrame(data_sf)\n",
    "df.index = ['p-value', 'Significance']\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing the canonical parameter**\n",
    "As briefly mentioned, there is an alternative way of calculating this, we have\n",
    "$$\n",
    "q_2 = \\frac{|\\varphi(\\hat\\theta) - \\varphi(\\hat{\\hat{\\theta}}) \\qquad \\varphi_{\\lambda}(\\hat{\\hat{\\theta}})|}{|\\varphi_{\\theta}(\\hat\\theta)|}\\left\\{\\frac{|j_{\\theta\\theta}(\\hat{\\theta})|}{|j_{\\lambda\\lambda}(\\hat{\\theta}_\\psi)|}\\right\\}^{1/2}\n",
    "$$\n",
    "where we use the canonical parameter $\\varphi$ instead. So now we try to find it!\n",
    "To recapitulate a bit, we have the transformed log-likelihood\n",
    "\\begin{equation}\n",
    "\\ell(\\theta,y) =  -(\\mu+b)+y\\ln(\\mu+b) -\\frac{1}{2}\\left(\\frac{b-b_0}{\\sigma_b}\\right)^2\n",
    "\\end{equation}\n",
    "For higher-order asymptotics, one often rewrites the log-likelihood as\n",
    "$$\n",
    "\\ell(\\theta;\\hat\\theta,a) =  \\varphi(\\theta)^Ts(y)-h(\\theta)+c(y)\n",
    "$$\n",
    "where $\\varphi(\\theta)$ is the **canonical parameter** that we are interested in, $s(y)$ is a *sufficient statistic*, $h(\\theta)$ captures additional terms dependent on $\\theta$, and $c(y)$ is a term independent of $\\theta$, that usually is ignored in inference. \n",
    "\n",
    "To construct the canonical parameter we can make use of the general formula\n",
    "$$\n",
    "\\varphi(\\theta) = ^T = \\sum_{i=1}^{n}\\frac{\\partial{\\ell(\\theta,y)}}{\\partial y_i}\\Big|_{y=y^0}V_i\n",
    "$$\n",
    "where we have that $n=1$ and $y^0=y$, and this $V_i$ is a vector that implement conditioning on an *approximately ancillary statistic*. This $V_i$ is generally constructed from pivotal quantities $z_i(y_i,\\theta)$, which always exists in the form of the probability integral transformation $F(y_i:\\theta)$, and can be found using the formula\n",
    "$$\n",
    "V = -\\left(\\frac{\\partial z}{\\partial y^T}\\right)^{-1}\\left(\\frac{\\partial z}{\\partial \\theta^T}\\right)\\Bigg|_{(y^0,\\hat\\theta_0)}\n",
    "$$\n",
    "So the name of the game is finding $z(y,\\theta)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One choice of the Pivot\n",
    "\n",
    "**(GPT recommendation following)** a natural choice since $y\\sim\\text{Poisson}(\\theta)$ is to use\n",
    "$$\n",
    "z(y,\\theta) = \\frac{y-(\\mu+b)}{\\sqrt{\\mu +b}}\n",
    "$$\n",
    "**WHICH APPLIES THE CLT FOR LARGE $\\mu+b$!!!** giving\n",
    "$$\n",
    "\\left(\\frac{\\partial z}{\\partial y}\\right)^{-1} = \\sqrt{\\mu +b }\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\left(\\frac{\\partial z}{\\partial \\theta}\\right) = \\frac{-y+(\\mu+b)}{2(\\mu+b)^{3/2}}\\begin{pmatrix}1\\\\1\\end{pmatrix}\n",
    "$$\n",
    "This was not pursued further, as this yields that $q_2=0$, meaning that no HOA is needed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another choice of the Pivot, from Anthony Davison:\n",
    "If we interpret this model as having an observation $b_0$ with an unknown mean $b$ and variance $\\sigma_b^2$ and a Poisson variable with mean $\\mu+b$, then one pivot is\n",
    "$$\n",
    "z_1=\\frac{b_0-b}{\\sigma_b^2}\n",
    "$$\n",
    "OR\n",
    "$$\n",
    "z_1=\\frac{b_0-b}{s_0}\n",
    "$$\n",
    "\n",
    "# **Not pursued further !!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\varphi(\\mu,b) = \\begin{pmatrix}\\ln(\\mu+b)\\\\\\ln(\\mu+b)\\end{pmatrix}\n",
    "$$ is a good canonical parameter, since $s(y)=y=\\hat\\mu+\\hat b+ a$ follows this form.\n",
    "\n",
    "$\\varphi(\\theta)^T = \\sum_{i=1}^{n}\\frac{\\partial{ell(\\theta,y)}}{\\partial y_i}\\Big|_{y=y^0}V_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding remaining parts\n",
    "From this it is trivial to find that\n",
    "$$\n",
    "\\varphi_\\theta(\\hat{\\theta}) = \\begin{pmatrix} \\frac{\\partial\\varphi}{\\partial\\mu}\\Big|_{\\theta=\\hat\\theta}\\\\\n",
    "\\frac{\\partial\\varphi}{\\partial b}\\Big|_{\\theta=\\hat\\theta}\\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{1}{\\hat\\mu+\\hat b}&&\\frac{1}{\\hat\\mu+\\hat b} \\\\ \\frac{1}{\\hat\\mu+\\hat b}&&\\frac{1}{\\hat\\mu+\\hat b}\\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{1}{y}&&\\frac{1}{y}\\\\\\frac{1}{y}&&\\frac{1}{y}\\end{pmatrix}\n",
    "$$\n",
    "and even easier now to see that\n",
    "$$\n",
    "\\varphi_\\lambda(\\hat{\\hat{\\theta}}) = \\begin{pmatrix}\\frac{1}{\\mu+\\hat{\\hat b}}\\\\\\frac{1}{\\mu+\\hat{\\hat b}}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all togheter\n",
    "That means that we now have\n",
    "$$\n",
    "|\\varphi(\\hat\\theta) - \\varphi(\\hat{\\hat{\\theta}}) \\qquad \\varphi_{\\lambda}(\\hat{\\hat{\\theta}})| = \\begin{vmatrix}\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right)& \\frac{1}{\\mu+\\hat{\\hat b}}\\\\\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right)& \\frac{1}{\\mu+\\hat{\\hat b}}\\end{vmatrix}\n",
    "$$\n",
    "This yields\n",
    "$$\n",
    "|\\varphi(\\hat\\theta) - \\varphi(\\hat{\\hat{\\theta}}) \\qquad \\varphi_{\\lambda}(\\hat{\\hat{\\theta}})| = \\sqrt{\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right)^2 + \\frac{1}{(\\mu+\\hat{\\hat b})^2}}\n",
    "$$\n",
    "Similarly we have\n",
    "$$\n",
    "\\varphi_\\theta(\\hat\\theta) = \\frac{\\sqrt{2}}{y^2}\n",
    "$$\n",
    "The fisher information term remains the same as for $q_1$ meaning that we have\n",
    "$$\n",
    "q_2 = \\frac{y^2\\sqrt{\\log\\left(\\frac{y}{\\mu+\\hat{\\hat b}}\\right)^2 + \\frac{1}{(\\mu+\\hat{\\hat b})^2}}}{\\sqrt{2}}\\left[\\frac{\\frac{1}{y\\sigma_b^2}}{\\frac{y}{(\\mu+\\hat{\\hat b})^2} + \\frac{1}{\\sigma_b^2}}\\right]^{1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_order_terms_2(mu, b_hhat, y, db):\n",
    "        numerator = y**2*np.sqrt(np.log(y/(mu+b_hhat))**2 + 1/(mu+b_hhat)**2 )\n",
    "        denominator = np.sqrt(2)\n",
    "        q = numerator/denominator\n",
    "        jacobian = np.sqrt(  np.abs(1/(y*db**2)) / np.abs(1/db**2 + (y/(mu+b_hhat)**2)))\n",
    "        return q*jacobian\n",
    "\n",
    "def modified_likelihood_root_HO_2(mu, mu_hat, b_hat, b_hhat, y, b_0, db):\n",
    "        r_mu = likelihood_root(mu, mu_hat, b_hat, b_hhat, y, b_0, db)\n",
    "        q_2 = higher_order_terms_2(mu, b_hhat, y, db)\n",
    "        return r_mu + (1 / r_mu) * np.log(q_2 / r_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "log_likelihood() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m r_s_HO2 = \u001b[43mmodified_likelihood_root_HO_2\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_hatt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m r_sf_HO2 = stats.norm.sf(r_s_HO, loc=\u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#Making a table of the results...\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmodified_likelihood_root_HO_2\u001b[39m\u001b[34m(mu, mu_hat, b_hat, b_hhat, y, b_0, db)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodified_likelihood_root_HO_2\u001b[39m(mu, mu_hat, b_hat, b_hhat, y, b_0, db):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         r_mu = \u001b[43mlikelihood_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_hhat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m         q_2 = higher_order_terms_2(mu, b_hhat, y, db)\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m r_mu + (\u001b[32m1\u001b[39m / r_mu) * np.log(q_2 / r_mu)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mlikelihood_root\u001b[39m\u001b[34m(mu, mu_hat, b_hat, b_hhat, y, b_0, db)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlikelihood_root\u001b[39m(mu, mu_hat, b_hat, b_hhat, y, b_0, db):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     ell_p_hat = \u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     ell_p = log_likelihood(mu, b_hhat, y, b_0, db)\n\u001b[32m      4\u001b[39m     r = np.sign(mu_hat- mu)*np.sqrt(\u001b[32m2\u001b[39m*(ell_p_hat-ell_p))\n",
      "\u001b[31mTypeError\u001b[39m: log_likelihood() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "r_s_HO2 = modified_likelihood_root_HO_2(0, mu_hat, b_hat, b_hatt, y_obs, b_0, sigma_b)\n",
    "\n",
    "r_sf_HO2 = stats.norm.sf(r_s_HO, loc=0)\n",
    "\n",
    "#Making a table of the results...\n",
    "\n",
    "data_sf = {'True value': [0.0264, 1.94],'r': [r_sf, r], 'r*': [r_star_sf, r_s], 'r* H.O.': [r_sf_HO, r_s_HO], 'r* H.O. canon': [r_sf_HO2, r_s_HO2]}\n",
    "df= pd.DataFrame(data_sf)\n",
    "df.index = ['p-value', 'Significance']\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that using $q_1$ yields a somewhat higher significance than just"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HODA_JAX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
